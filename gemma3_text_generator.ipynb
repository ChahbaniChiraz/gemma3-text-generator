!pip install -q transformers accelerate torch huggingface_hub

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import torch

# Authenticate with Hugging Face token 
login("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")

model_id = "google/gemma-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    use_auth_token=True
)

prompt = input("üìù Entrez votre question ici : ")

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)
