{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc01af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prompt the user for input\n",
    "prompt = input(\"üìù Entrez votre question ici : \")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate and stream the output\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
